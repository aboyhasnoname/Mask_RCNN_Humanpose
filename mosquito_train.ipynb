{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations Superlee:\n",
      "BACKBONE_SHAPES                [[256 256]\n",
      " [128 128]\n",
      " [ 64  64]\n",
      " [ 32  32]\n",
      " [ 16  16]]\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "GPU_COUNT                      1\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_MIN_DIM                  800\n",
      "IMAGE_PADDING                  True\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "KEYPOINT_MASK_POOL_SIZE        7\n",
      "KEYPOINT_MASK_SHAPE            [56, 56]\n",
      "KEYPOINT_THRESHOLD             0.005\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.002\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               128\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                [256, 256]\n",
      "NAME                           mosquitoes\n",
      "NUM_CLASSES                    3\n",
      "NUM_KEYPOINTS                  3\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    150\n",
      "STEPS_PER_EPOCH                100\n",
      "TRAIN_ROIS_PER_IMAGE           50\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "WEIGHT_LOSS                    True\n",
      "\n",
      "\n",
      "train  num_images:  360\n",
      "val  num_images:  43\n",
      "['input_image', 'zero_padding2d_2', 'conv1', 'bn_conv1', 'activation_68', 'max_pooling2d_2', 'res2a_branch2a', 'bn2a_branch2a', 'activation_69', 'res2a_branch2b', 'bn2a_branch2b', 'activation_70', 'res2a_branch2c', 'res2a_branch1', 'bn2a_branch2c', 'bn2a_branch1', 'add_33', 'res2a_out', 'res2b_branch2a', 'bn2b_branch2a', 'activation_71', 'res2b_branch2b', 'bn2b_branch2b', 'activation_72', 'res2b_branch2c', 'bn2b_branch2c', 'add_34', 'res2b_out', 'res2c_branch2a', 'bn2c_branch2a', 'activation_73', 'res2c_branch2b', 'bn2c_branch2b', 'activation_74', 'res2c_branch2c', 'bn2c_branch2c', 'add_35', 'res2c_out', 'res3a_branch2a', 'bn3a_branch2a', 'activation_75', 'res3a_branch2b', 'bn3a_branch2b', 'activation_76', 'res3a_branch2c', 'res3a_branch1', 'bn3a_branch2c', 'bn3a_branch1', 'add_36', 'res3a_out', 'res3b_branch2a', 'bn3b_branch2a', 'activation_77', 'res3b_branch2b', 'bn3b_branch2b', 'activation_78', 'res3b_branch2c', 'bn3b_branch2c', 'add_37', 'res3b_out', 'res3c_branch2a', 'bn3c_branch2a', 'activation_79', 'res3c_branch2b', 'bn3c_branch2b', 'activation_80', 'res3c_branch2c', 'bn3c_branch2c', 'add_38', 'res3c_out', 'res3d_branch2a', 'bn3d_branch2a', 'activation_81', 'res3d_branch2b', 'bn3d_branch2b', 'activation_82', 'res3d_branch2c', 'bn3d_branch2c', 'add_39', 'res3d_out', 'res4a_branch2a', 'bn4a_branch2a', 'activation_83', 'res4a_branch2b', 'bn4a_branch2b', 'activation_84', 'res4a_branch2c', 'res4a_branch1', 'bn4a_branch2c', 'bn4a_branch1', 'add_40', 'res4a_out', 'res4b_branch2a', 'bn4b_branch2a', 'activation_85', 'res4b_branch2b', 'bn4b_branch2b', 'activation_86', 'res4b_branch2c', 'bn4b_branch2c', 'add_41', 'res4b_out', 'res4c_branch2a', 'bn4c_branch2a', 'activation_87', 'res4c_branch2b', 'bn4c_branch2b', 'activation_88', 'res4c_branch2c', 'bn4c_branch2c', 'add_42', 'res4c_out', 'res4d_branch2a', 'bn4d_branch2a', 'activation_89', 'res4d_branch2b', 'bn4d_branch2b', 'activation_90', 'res4d_branch2c', 'bn4d_branch2c', 'add_43', 'res4d_out', 'res4e_branch2a', 'bn4e_branch2a', 'activation_91', 'res4e_branch2b', 'bn4e_branch2b', 'activation_92', 'res4e_branch2c', 'bn4e_branch2c', 'add_44', 'res4e_out', 'res4f_branch2a', 'bn4f_branch2a', 'activation_93', 'res4f_branch2b', 'bn4f_branch2b', 'activation_94', 'res4f_branch2c', 'bn4f_branch2c', 'add_45', 'res4f_out', 'res4g_branch2a', 'bn4g_branch2a', 'activation_95', 'res4g_branch2b', 'bn4g_branch2b', 'activation_96', 'res4g_branch2c', 'bn4g_branch2c', 'add_46', 'res4g_out', 'res4h_branch2a', 'bn4h_branch2a', 'activation_97', 'res4h_branch2b', 'bn4h_branch2b', 'activation_98', 'res4h_branch2c', 'bn4h_branch2c', 'add_47', 'res4h_out', 'res4i_branch2a', 'bn4i_branch2a', 'activation_99', 'res4i_branch2b', 'bn4i_branch2b', 'activation_100', 'res4i_branch2c', 'bn4i_branch2c', 'add_48', 'res4i_out', 'res4j_branch2a', 'bn4j_branch2a', 'activation_101', 'res4j_branch2b', 'bn4j_branch2b', 'activation_102', 'res4j_branch2c', 'bn4j_branch2c', 'add_49', 'res4j_out', 'res4k_branch2a', 'bn4k_branch2a', 'activation_103', 'res4k_branch2b', 'bn4k_branch2b', 'activation_104', 'res4k_branch2c', 'bn4k_branch2c', 'add_50', 'res4k_out', 'res4l_branch2a', 'bn4l_branch2a', 'activation_105', 'res4l_branch2b', 'bn4l_branch2b', 'activation_106', 'res4l_branch2c', 'bn4l_branch2c', 'add_51', 'res4l_out', 'res4m_branch2a', 'bn4m_branch2a', 'activation_107', 'res4m_branch2b', 'bn4m_branch2b', 'activation_108', 'res4m_branch2c', 'bn4m_branch2c', 'add_52', 'res4m_out', 'res4n_branch2a', 'bn4n_branch2a', 'activation_109', 'res4n_branch2b', 'bn4n_branch2b', 'activation_110', 'res4n_branch2c', 'bn4n_branch2c', 'add_53', 'res4n_out', 'res4o_branch2a', 'bn4o_branch2a', 'activation_111', 'res4o_branch2b', 'bn4o_branch2b', 'activation_112', 'res4o_branch2c', 'bn4o_branch2c', 'add_54', 'res4o_out', 'res4p_branch2a', 'bn4p_branch2a', 'activation_113', 'res4p_branch2b', 'bn4p_branch2b', 'activation_114', 'res4p_branch2c', 'bn4p_branch2c', 'add_55', 'res4p_out', 'res4q_branch2a', 'bn4q_branch2a', 'activation_115', 'res4q_branch2b', 'bn4q_branch2b', 'activation_116', 'res4q_branch2c', 'bn4q_branch2c', 'add_56', 'res4q_out', 'res4r_branch2a', 'bn4r_branch2a', 'activation_117', 'res4r_branch2b', 'bn4r_branch2b', 'activation_118', 'res4r_branch2c', 'bn4r_branch2c', 'add_57', 'res4r_out', 'res4s_branch2a', 'bn4s_branch2a', 'activation_119', 'res4s_branch2b', 'bn4s_branch2b', 'activation_120', 'res4s_branch2c', 'bn4s_branch2c', 'add_58', 'res4s_out', 'res4t_branch2a', 'bn4t_branch2a', 'activation_121', 'res4t_branch2b', 'bn4t_branch2b', 'activation_122', 'res4t_branch2c', 'bn4t_branch2c', 'add_59', 'res4t_out', 'res4u_branch2a', 'bn4u_branch2a', 'activation_123', 'res4u_branch2b', 'bn4u_branch2b', 'activation_124', 'res4u_branch2c', 'bn4u_branch2c', 'add_60', 'res4u_out', 'res4v_branch2a', 'bn4v_branch2a', 'activation_125', 'res4v_branch2b', 'bn4v_branch2b', 'activation_126', 'res4v_branch2c', 'bn4v_branch2c', 'add_61', 'res4v_out', 'res4w_branch2a', 'bn4w_branch2a', 'activation_127', 'res4w_branch2b', 'bn4w_branch2b', 'activation_128', 'res4w_branch2c', 'bn4w_branch2c', 'add_62', 'res4w_out', 'res5a_branch2a', 'bn5a_branch2a', 'activation_129', 'res5a_branch2b', 'bn5a_branch2b', 'activation_130', 'res5a_branch2c', 'res5a_branch1', 'bn5a_branch2c', 'bn5a_branch1', 'add_63', 'res5a_out', 'res5b_branch2a', 'bn5b_branch2a', 'activation_131', 'res5b_branch2b', 'bn5b_branch2b', 'activation_132', 'res5b_branch2c', 'bn5b_branch2c', 'add_64', 'res5b_out', 'res5c_branch2a', 'bn5c_branch2a', 'activation_133', 'res5c_branch2b', 'bn5c_branch2b', 'activation_134', 'res5c_branch2c', 'bn5c_branch2c', 'add_65', 'res5c_out', 'fpn_c5p5', 'fpn_p5upsampled', 'fpn_c4p4', 'fpn_p4add', 'fpn_p4upsampled', 'fpn_c3p3', 'fpn_p3add', 'fpn_p3upsampled', 'fpn_c2p2', 'fpn_p2add', 'fpn_p5', 'fpn_p2', 'fpn_p3', 'fpn_p4', 'fpn_p6', 'rpn_model', 'rpn_class', 'rpn_bbox', 'input_gt_boxes', 'input_2', 'ROI', 'input_gt_class_ids', 'gt_boxes', 'gt_keypoints', 'input_gt_masks', 'proposal_targets', 'roi_align_keypoint_mask', 'mrcnn_keypoint_mask_conv1', 'mrcnn_keypoint_mask_bn1', 'activation_142', 'mrcnn_keypoint_mask_conv2', 'mrcnn_keypoint_mask_bn2', 'activation_143', 'mrcnn_keypoint_mask_conv3', 'mrcnn_keypoint_mask_bn3', 'activation_144', 'mrcnn_keypoint_mask_conv4', 'mrcnn_keypoint_mask_bn4', 'activation_145', 'mrcnn_keypoint_mask_conv5', 'mrcnn_keypoint_mask_bn5', 'activation_146', 'roi_align_mask', 'mrcnn_keypoint_mask_conv6', 'mrcnn_mask_conv1', 'mrcnn_keypoint_mask_bn6', 'mrcnn_mask_bn1', 'activation_147', 'activation_138', 'mrcnn_keypoint_mask_conv7', 'mrcnn_mask_conv2', 'roi_align_classifier', 'mrcnn_keypoint_mask_bn7', 'mrcnn_mask_bn2', 'mrcnn_class_conv1', 'activation_148', 'activation_139', 'mrcnn_class_bn1', 'mrcnn_keypoint_mask_conv8', 'mrcnn_mask_conv3', 'activation_135', 'mrcnn_keypoint_mask_bn8', 'mrcnn_mask_bn3', 'mrcnn_class_conv2', 'activation_149', 'activation_140', 'mrcnn_class_bn2', 'mrcnn_keypoint_mask_deconv', 'mrcnn_mask_conv4', 'activation_136', 'mrcnn_keypoint_mask_upsample_1', 'mrcnn_mask_bn4', 'pool_squeeze', 'mrcnn_keypoint_mask_upsample_2', 'activation_141', 'mrcnn_bbox_fc', 'mrcnn_keypoint_mask_transpose', 'input_image_meta', 'mrcnn_mask_deconv', 'rpn_class_logits', 'mrcnn_class_logits', 'mrcnn_bbox', 'mrcnn_keypoint_mask_reshape', 'input_rpn_match', 'input_rpn_bbox', 'lambda_3', 'mrcnn_mask', 'mrcnn_class', 'output_rois', 'rpn_class_loss', 'rpn_bbox_loss', 'mrcnn_class_loss', 'mrcnn_bbox_loss', 'keypoint_mrcnn_mask_loss', 'mrcnn_mask_loss']\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'saving'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f09ccb640737>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;31m# Load weights trained on MS-COCO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOCO_MODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mrcnn_class_logits\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mrcnn_bbox_fc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mrcnn_bbox\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mrcnn_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading weights from \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCOCO_MODEL_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jm1/Mask_RCNN_Humanpose/model.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, exclude)\u001b[0m\n\u001b[1;32m   2839\u001b[0m         \"\"\"\n\u001b[1;32m   2840\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2841\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msaving\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2843\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'saving'"
     ]
    }
   ],
   "source": [
    "# %load mosquitoes_train.py\n",
    "import json\n",
    "import skimage\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import coco\n",
    "import utils\n",
    "import model as modellib\n",
    "#import visualize\n",
    "from model import log\n",
    "dataset_dir = './data'\n",
    "annotations = \"via_region_data.json\"\n",
    "\n",
    "class MosquitoesConfig(coco.Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"mosquitoes\"\n",
    "\n",
    "    # We use a GPU with 12GB memory, which can fit two images.\n",
    "    # Adjust down if you use a smaller GPU.\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    # Uncomment to train on 8 GPUs (default is 1)\n",
    "    #GPU_COUNT = 2\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    # NUM_CLASSES = 1 + 80  # COCO has 80 classes\n",
    "    NUM_CLASSES = 1 + 2  # Person and background\n",
    "\n",
    "    NUM_KEYPOINTS = 3\n",
    "    MINI_MASK_SHAPE = [256, 256]\n",
    "    MASK_SHAPE = [28, 28]\n",
    "    KEYPOINT_MASK_SHAPE = [56,56]\n",
    "    # DETECTION_MAX_INSTANCES = 50\n",
    "    TRAIN_ROIS_PER_IMAGE = 50\n",
    "    MAX_GT_INSTANCES = 128\n",
    "    RPN_TRAIN_ANCHORS_PER_IMAGE = 150\n",
    "    USE_MINI_MASK = True\n",
    "    MASK_POOL_SIZE = 14\n",
    "    KEYPOINT_MASK_POOL_SIZE = 7\n",
    "    LEARNING_RATE = 0.002\n",
    "    STEPS_PER_EPOCH = 100\n",
    "    WEIGHT_LOSS = True\n",
    "    KEYPOINT_THRESHOLD = 0.005\n",
    "\n",
    "config = MosquitoesConfig()\n",
    "config.display()\n",
    "\n",
    "\n",
    "class MosquitoesDataset(utils.Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__(MosquitoesDataset)\n",
    "        num_classes = 3\n",
    "        self.task_type = \"person_keypoints\"\n",
    "        # the connection between 2 close keypoints\n",
    "        self._skeleton = []\n",
    "        # keypoint names\n",
    "        # [\"prob\",\"head\",\"tail\"]\n",
    "        self._keypoint_names = []\n",
    "\n",
    "    def load_dataset(self, dataset_dir, subset='train'):\n",
    "        \"\"\"Load a subset of the Balloon dataset.\n",
    "        dataset_dir: Root directory of the dataset.\n",
    "        subset: Subset to load: train or val\n",
    "        \"\"\"\n",
    "        # Add classes. We have only one class to add.\n",
    "#         self.add_class(\"balloon\", 1, \"balloon\")\n",
    "\n",
    "#         # Train or validation dataset?\n",
    "        assert subset in [\"train\", \"val\"]\n",
    "#         dataset_dir = os.path.join(dataset_dir, subset)\n",
    "\n",
    "        # Load annotations\n",
    "        # VGG Image Annotator saves each image in the form:\n",
    "        # { 'filename': '28503151_5b5b7ec140_b.jpg',\n",
    "        #   'regions': {\n",
    "        #       '0': {\n",
    "        #           'region_attributes': {},\n",
    "        #           'shape_attributes': {\n",
    "        #               'all_points_x': [...],\n",
    "        #               'all_points_y': [...],\n",
    "        #               'name': 'polygon'}},\n",
    "        #       ... more regions ...\n",
    "        #   },\n",
    "        #   'size': 100202\n",
    "        # }\n",
    "        # We mostly care about the x and y coordinates of each region\n",
    "        if subset=='train':\n",
    "             annotations = json.load(open(os.path.join(dataset_dir, \"annotations\",\"train.json\")))\n",
    "        else:\n",
    "             annotations = json.load(open(os.path.join(dataset_dir, \"annotations\",\"val.json\")))\n",
    "\n",
    "        annotations = list(annotations.values())  # don't need the dict keys\n",
    "\n",
    "        # The VIA tool saves images in the JSON even if they don't have any\n",
    "        # annotations. Skip unannotated images.\n",
    "        annotations = [a for a in annotations if a['regions']]\n",
    "\n",
    "        #remove wrong annotations\n",
    "        remove_index = 0\n",
    "        for index, a in enumerate(annotations):\n",
    "            if a['filename']=='8_0083_4.jpg':\n",
    "                remove_index = index\n",
    "        annotations.pop(remove_index)\n",
    "        \n",
    "        if subset=='train':\n",
    "            annotations = annotations[:360]\n",
    "        else:\n",
    "            annotations = annotations[360:]\n",
    "        print(subset,' num_images: ',len(annotations))\n",
    "        \n",
    "        # Add images\n",
    "        for a in annotations:\n",
    "            # load_mask() needs the image size to convert polygons to masks.\n",
    "            # Unfortunately, VIA doesn't include it in JSON, so we must read\n",
    "            # the image. This is only managable since the dataset is tiny.\n",
    "\n",
    "            image_path = os.path.join(dataset_dir, \"images\", a['filename'])\n",
    "            image = skimage.io.imread(image_path)\n",
    "            height, width = image.shape[:2]\n",
    "            num_mosquitoes = 0\n",
    "            cl = list()\n",
    "            bb = list()\n",
    "            kp = list()\n",
    "            for index, attr in enumerate(a['regions']):\n",
    "                #print(index,attr)\n",
    "                if (index%5==0):\n",
    "                    #attr['region_attributes'].setdefault('class','1')# if the 'class' is missing, fill out automatically, but this error seldom happens\n",
    "                    if not 'class' in attr['region_attributes']:\n",
    "                        attr['region_attributes'].setdefault('class','1')# if the 'class' is missing, fill out automatically, but this error seldom happens\n",
    "                    if not attr['region_attributes']['class']:#if the va;ues of 'class' is missing, fill out.\n",
    "                        cl.append(str(random.randint(1,2)))\n",
    "                    if attr['region_attributes']['class']:\n",
    "                        cl.append(int(attr['region_attributes']['class']))\n",
    "                    if 'y' in attr['shape_attributes']:# eror: if a extral point is marked without sense\n",
    "                        bb.append([attr['shape_attributes']['y'], attr['shape_attributes']['x'], attr['shape_attributes']['height'], attr['shape_attributes']['width']])\n",
    "\n",
    "                elif (index%5==1):\n",
    "                    pass\n",
    "                else:\n",
    "                    kp.append(( attr['shape_attributes']['cy'], attr['shape_attributes']['cx']))\n",
    "\n",
    "                num_mosquitoes += 1\n",
    "\n",
    "            num_mosquitoes = int(num_mosquitoes/5)\n",
    "\n",
    "            self.add_image(\n",
    "                \"mosquitoes\",\n",
    "                image_id=a['filename'],  # use file name as a unique image id\n",
    "                path=image_path,\n",
    "                width=width, height=height,\n",
    "                cl = cl,\n",
    "                bounding_box = bb,\n",
    "                key_points = kp,\n",
    "                num_mosquitoes = num_mosquitoes\n",
    "                )\n",
    "\n",
    "    def load_bbox(self, image_id):\n",
    "        bounding_box = self.image_info[image_id]['bounding_box']\n",
    "        num_mosquitoes = self.image_info[image_id]['num_mosquitoes']\n",
    "        bounding_box = np.reshape(bounding_box, (-1,4))\n",
    "        bounding_box[:,2] += bounding_box[:,0]\n",
    "        bounding_box[:,3] += bounding_box[:,1]\n",
    "        return bounding_box\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        image_path = self.image_info[image_id]['path']\n",
    "        image = skimage.io.imread(image_path)\n",
    "        return image\n",
    "\n",
    "    def load_keypoints(self, image_id):\n",
    "        \"\"\"Load person keypoints for the given image.\n",
    "\n",
    "        Returns:\n",
    "        key_points: num_keypoints coordinates and visibility (x,y,v)  [num_person,num_keypoints,3] of num_person\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks, here is always equal to [num_person, 1]\n",
    "        \"\"\"\n",
    "        # If not a COCO image, delegate to parent class.\n",
    "#         print(self.image_info)\n",
    "#         image_info = self.image_info[image_id]\n",
    "#         if image_info[\"source\"] != \"coco\":\n",
    "#             return super(CocoDataset, self).load_mask(image_id)\n",
    "\n",
    "        keypoints = []\n",
    "        class_ids = []\n",
    "        instance_masks = []\n",
    "        info = self.image_info[image_id]\n",
    "        num_mosquitoes = info['num_mosquitoes']\n",
    "        # Build mask of shape [height, width, instance_count] and list\n",
    "        # of class IDs that correspond to each channel of the mask.\n",
    "        for index in range(0, int(info['num_mosquitoes'])):\n",
    "            class_id = info['cl'][index]\n",
    "\n",
    "            m = np.zeros((info['height'], info['width']), dtype=np.uint8)\n",
    "            # generate masks\n",
    "            for m_index in range(0,3):\n",
    "                #m_index = index*3 + m_index\n",
    "                #m = np.zeros((info['height'], info['width']), dtype=np.uint8)\n",
    "                x = info['key_points'][m_index][0]\n",
    "                y = info['key_points'][m_index][1]\n",
    "                m[x,y] = 255\n",
    "            instance_masks.append(m)\n",
    "            #load keypoints\n",
    "            keypoints = info[\"key_points\"]\n",
    "            keypoints = np.reshape(keypoints,(-1,2))\n",
    "            new_col = np.ones((keypoints.shape[0],1))+1\n",
    "            keypoints = np.hstack((keypoints, new_col))\n",
    "            keypoints = np.reshape(keypoints, (num_mosquitoes,3,3))\n",
    "#             keypoints.append(keypoint)\n",
    "            class_ids.append(class_id)\n",
    "        # Pack instance masks into an array\n",
    "#         if class_ids:\n",
    "        keypoints = np.array(keypoints,dtype=np.int32)\n",
    "        class_ids = np.array(class_ids, dtype=np.int32)\n",
    "        masks = np.stack(instance_masks, axis=2)\n",
    "\n",
    "        return keypoints, masks, class_ids\n",
    "#         else:\n",
    "#             # Call super class to return an empty mask\n",
    "#             return super(CocoDataset, self).load_keypoints(image_id)\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the path of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        return info[\"path\"]\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "\n",
    "    train_dataset_keypoints = MosquitoesDataset()\n",
    "    train_dataset_keypoints.load_dataset(dataset_dir, \"train\")\n",
    "    train_dataset_keypoints.prepare()\n",
    "\n",
    "    val_dataset_keypoints = MosquitoesDataset()\n",
    "    val_dataset_keypoints.load_dataset(dataset_dir, \"val\")\n",
    "    val_dataset_keypoints.prepare()\n",
    "\n",
    "    ROOT_DIR = os.getcwd()\n",
    "    MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "    config = MosquitoesConfig()\n",
    "# Local path to trained weights file\n",
    "    COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Create model object in inference mode.\n",
    "    model = modellib.MaskRCNN(mode=\"training\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "    print(\"Loading weights from \", COCO_MODEL_PATH)\n",
    "\n",
    "# Training - Stage 1\n",
    "    print(\"Train heads\")\n",
    "    model.train(train_dataset_keypoints, val_dataset_keypoints,\\\n",
    "            learning_rate=config.LEARNING_RATE,\\\n",
    "            epochs=15,\\\n",
    "            layers=\"heads\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_tf_19_keras216",
   "language": "python",
   "name": "tensorflow_tf_19_keras216"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
